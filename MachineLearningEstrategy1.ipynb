{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np #Mathematical tools\n",
    "import matplotlib.pyplot as plt #Plots charts\n",
    "import pandas as pd #Import and manage data sets\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Preprocessing library\n",
    "from sklearn.preprocessing import Imputer\n",
    "# Categorical data encoder library\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# Splitting the data set into training and testing sets library\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Feature scaling library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports part 2\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions part 1\n",
    "\n",
    "def best_output(row):\n",
    "    # se o downloadTime é diferente de 100 para T2 e T3, ambos os targets completam o download\n",
    "    if ((row.downloadTimeT2!=100)&(row.downloadTimeT3!=100)):\n",
    "        # o melhor output será aquele para o qual o downloadTime é menor\n",
    "        if (row.downloadTimeT2<=row.downloadTimeT3):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    # se o downloadTime é diferente de 100 apenas para um dos targets, só um deles completa o download\n",
    "    elif ((row.downloadTimeT2!=100)|(row.downloadTimeT3!=100)):\n",
    "        # o melhor output será aquele para o qual o downloadTime é diferente de 100\n",
    "        if (row.downloadTimeT2!=100):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    # se o downloadTime é igual a 100 para T2 e T3, ambos os targets não completam o download\n",
    "    elif ((row.downloadTimeT2==100)&(row.downloadTimeT3==100)):\n",
    "        # o melhor output será aquele para o qual o rxBytes é maior\n",
    "        if (row.rxBytesT2>=row.rxBytesT3):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "def download_complete(row, column):\n",
    "    if (row[column]==0):\n",
    "        if(row.downloadTimeT2!=100.0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if(row.downloadTimeT3!=100.0):\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "        \n",
    "def download_time(row, column):\n",
    "    if ((row[column]==0)&(row.downloadTimeT2<100)):\n",
    "        return row.downloadTimeT2\n",
    "    elif ((row[column]==1)&(row.downloadTimeT3<100)):\n",
    "        return row.downloadTimeT3\n",
    "\n",
    "    \n",
    "def throughput(row, column):\n",
    "    if (row[column]==0):\n",
    "        tp = (row.rxBytesT2/row.downloadTimeT2)*8/1e6\n",
    "        return tp\n",
    "    else:\n",
    "        tp = (row.rxBytesT3/row.downloadTimeT3)*8/1e6\n",
    "        return tp\n",
    "    \n",
    "def optimum_choice(row, column):\n",
    "    if (row[column]==row.best_output):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def a2a4rsrp(row):\n",
    "    if (row.rsrp2>=row.rsrp3):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions part 2\n",
    "def percentageOfErrors(y_real, y_predicted):\n",
    "    p= ((y_predicted != y_real).sum()/ y_real.size)*100\n",
    "    return p \n",
    "\n",
    "def gettingEstimatorConfiguration(estimator, param_test,x_train, y_train, score):\n",
    "    conf=GridSearchCV(estimator=estimator, param_grid=param_test, scoring=score, cv=10, n_jobs=-1)\n",
    "    conf.fit(x_train, y_train)\n",
    "    print(conf.best_estimator_)\n",
    "    return conf.best_estimator_\n",
    "\n",
    "def gettingTheConfigurationOfAllEstimatorsOfAType(estimators, param_test, train):\n",
    "    x=np.array(train[0])\n",
    "    y=np.array(train[1:3])\n",
    "    # Level 2A\n",
    "    est10=gettingEstimatorConfiguration(estimators[1,0], param_test,x[1], y[0,1],'neg_mean_squared_error')\n",
    "    est11=gettingEstimatorConfiguration(estimators[1,1], param_test,x[1], y[1,1],'neg_mean_squared_error')    \n",
    "    # Level 2B\n",
    "    est20=gettingEstimatorConfiguration(estimators[2,0], param_test,x[2], y[0,2],'neg_mean_squared_error')\n",
    "    est21=gettingEstimatorConfiguration(estimators[2,1], param_test,x[2], y[1,2],'neg_mean_squared_error')\n",
    "    # Level 1\n",
    "    est00=gettingEstimatorConfiguration(estimators[0,0], param_test,x[0], y[0,0],'accuracy')\n",
    "    est01=gettingEstimatorConfiguration(estimators[0,1], param_test,x[0], y[1,0], 'accuracy')\n",
    "\n",
    "    return [[est00,est01],[est10,est11],[est20, est21]]\n",
    "\n",
    "def gettingAllEstimatorsConfigurations(train, randomState):\n",
    "    # All test parameters\n",
    "    param_test_knn = [{'n_neighbors':[(i) for i in range(5,31)], 'p':[1,2],'weights':['uniform', 'distance'],\n",
    "                  'algorithm':['auto', 'brute']},{'n_neighbors':[(i) for i in range(5,31)], 'p':[1,2],\n",
    "                 'weights':['uniform', 'distance'],\n",
    "                  'algorithm':['ball_tree', 'kd_tree'], 'leaf_size':[10,20,30,40,50,60,70, 80,90,100]} ]\n",
    "    param_test_svm =[{'C':[1,2,3,4,5,6,7,8,9,10,100,200, 300, 400, 500, 600, 700, 800, 900, 1000],'degree':[(i) for i in range(1,21)],'kernel':['poly'],'decision_function_shape':['ovo','ovr'],\n",
    "                  'shrinking':[True, False]},\n",
    "                 {'C':[1,2,3,4,5,6,7,8,9,10,100,200, 300, 400, 500, 600, 700, 800, 900, 1000],'kernel':['linear'],'decision_function_shape':['ovo','ovr'],\n",
    "                  'shrinking':[True, False]},\n",
    "                  {'C':[1,2,3,4,5,6,7,8,9,10,100,200, 300, 400, 500, 600, 700, 800, 900, 1000],'kernel':['rbf', 'sigmoid'],'gamma':[1,0.5,0.1, 0.01,0.001, 0.0001, 0.0000001],\n",
    "                     'shrinking':[True, False]}] \n",
    "    # Creating the DT estimator parameters\n",
    "    param_test_dt=[{ 'splitter':['best', 'random'], 'max_features':[None,'auto', 'sqrt', 'log2']}]    \n",
    "   \n",
    "    # Creating the RF estimator parameters\n",
    "    param_test_rf = {'n_estimators':[(i) for i in range(5,31)],  'max_features':[None,'auto', 'sqrt', 'log2'],  \n",
    "                     'verbose':[0], 'n_jobs':[-1], 'bootstrap':[True, False]}\n",
    "    \n",
    "    hl=[[12, 6, 3], [24, 12, 6],[10, 10, 10],[20,20,20],[30,30,30], [60], [120]]\n",
    "   \n",
    "    #param_test_nn = [{'activation' : ['identity', 'logistic', 'tanh', 'relu'], 'solver' :['lbfgs'],\n",
    "     #           'learning_rate' : ['constant', 'invscaling', 'adaptive'], 'hidden_layer_sizes': hl,\n",
    "     #                'max_iter':[500]},{'activation' : ['identity', 'logistic', 'tanh', 'relu'], 'solver' :['sgd', 'adam'],\n",
    "     #           'learning_rate' : ['constant', 'invscaling', 'adaptive'], 'hidden_layer_sizes': hl,\n",
    "     #                'max_iter':[500], 'early_stopping':[True, False], 'validation_fraction':[0.1,0.15,0.2],'shuffle':[True, False],\n",
    "    #                                   'learning_rate_init':[0.001,0.01, 0.1]}]\n",
    "    param_test_nn = [{'activation' : ['identity', 'logistic', 'tanh', 'relu'], 'solver' :['lbfgs'],\n",
    "               'learning_rate' : ['constant', 'invscaling', 'adaptive'], 'hidden_layer_sizes': hl,\n",
    "                    'max_iter':[400]}]   \n",
    "        \n",
    "    # All estimators\n",
    "    # Level 1\n",
    "    estimator_knn00=KNeighborsClassifier(metric = 'minkowski')\n",
    "    estimator_svm00=SVC(random_state=randomState)\n",
    "    estimator_dt00=DecisionTreeClassifier(random_state=randomState)\n",
    "    estimator_rf00=RandomForestClassifier(random_state=randomState)\n",
    "    estimator_nn00=MLPClassifier(random_state=randomState)\n",
    "    \n",
    "    estimator_knn01=KNeighborsClassifier(metric = 'minkowski')\n",
    "    estimator_svm01=SVC(random_state=randomState)\n",
    "    estimator_dt01=DecisionTreeClassifier(random_state=randomState)\n",
    "    estimator_rf01=RandomForestClassifier(random_state=randomState)\n",
    "    estimator_nn01=MLPClassifier(random_state=randomState)\n",
    "    \n",
    "    # Level 2A\n",
    "    estimator_knn10=KNeighborsRegressor(metric = 'minkowski')\n",
    "    estimator_svm10=SVR()\n",
    "    estimator_dt10=DecisionTreeRegressor(random_state=randomState)\n",
    "    estimator_rf10=RandomForestRegressor(random_state=randomState)\n",
    "    estimator_nn10=MLPRegressor(random_state=randomState)\n",
    "    \n",
    "    estimator_knn11=KNeighborsRegressor(metric = 'minkowski')\n",
    "    estimator_svm11=SVR()\n",
    "    estimator_dt11=DecisionTreeRegressor(random_state=randomState)\n",
    "    estimator_rf11=RandomForestRegressor(random_state=randomState)\n",
    "    estimator_nn11=MLPRegressor(random_state=randomState)\n",
    "    \n",
    "    # Level 2B  \n",
    "    estimator_knn20=KNeighborsRegressor(metric = 'minkowski')\n",
    "    estimator_svm20=SVR(0)\n",
    "    estimator_dt20=DecisionTreeRegressor(random_state=randomState)\n",
    "    estimator_rf20=RandomForestRegressor(random_state=randomState)\n",
    "    estimator_nn20=MLPRegressor(random_state=randomState)\n",
    "    \n",
    "    estimator_knn21=KNeighborsRegressor(metric = 'minkowski')\n",
    "    estimator_svm21=SVR(0)\n",
    "    estimator_dt21=DecisionTreeRegressor(random_state=randomState)\n",
    "    estimator_rf21=RandomForestRegressor(random_state=randomState)\n",
    "    estimator_nn21=MLPRegressor(random_state=randomState)\n",
    "    \n",
    "    estimators_knn=np.array([[estimator_knn00,estimator_knn01],[estimator_knn10,estimator_knn11],\n",
    "                    [estimator_knn20,estimator_knn21]])\n",
    "    estimators_nn=np.array([[estimator_nn00,estimator_nn01],[estimator_nn10,estimator_nn11],\n",
    "                   [estimator_nn20,estimator_nn21]])\n",
    "    estimators_svm=np.array([[estimator_svm00,estimator_svm01],[estimator_svm10,estimator_svm11],\n",
    "                   [estimator_svm20,estimator_svm21]])\n",
    "    estimators_dt=np.array([[estimator_dt00,estimator_dt01],[estimator_dt10,estimator_dt11],\n",
    "                   [estimator_dt20,estimator_dt21]])\n",
    "    estimators_rf=np.array([[estimator_rf00,estimator_rf01],[estimator_rf10,estimator_rf11],\n",
    "                   [estimator_rf20,estimator_rf21]])\n",
    "    \n",
    "    #Finding the best configurations\n",
    "    \n",
    "    print(\"Best parameters for NN estimator\")\n",
    "    conf_nn=gettingTheConfigurationOfAllEstimatorsOfAType(estimators=estimators_nn, param_test=param_test_nn, \n",
    "                                                          train=train)   \n",
    "    print(\"Best parameters for KNN estimator\")\n",
    "    conf_knn=gettingTheConfigurationOfAllEstimatorsOfAType(estimators=estimators_knn, param_test=param_test_knn, \n",
    "                                                          train=train)\n",
    "    print(\"Best parameters for SVM estimator\")\n",
    "    conf_svm=gettingTheConfigurationOfAllEstimatorsOfAType(estimators=estimators_svm, param_test=param_test_svm, \n",
    "                                                          train=train)\n",
    "    print(\"Best parameters for DT estimator\")\n",
    "    conf_dt=gettingTheConfigurationOfAllEstimatorsOfAType(estimators=estimators_dt, param_test=param_test_dt, \n",
    "                                                          train=train)\n",
    "    print(\"Best parameters for RF estimator\")\n",
    "    conf_rf=gettingTheConfigurationOfAllEstimatorsOfAType(estimators=estimators_rf, param_test=param_test_rf, \n",
    "                                                          train=train)\n",
    "    \n",
    "    \n",
    "    conf=[conf_knn,conf_svm, conf_dt, conf_rf, conf_nn]\n",
    "    return conf\n",
    "\n",
    "def theBestScore(oldBest, candidate):\n",
    "    if oldBest>candidate:\n",
    "        return oldBest\n",
    "    else:\n",
    "        return candidate    \n",
    "def theWorstScore(oldWorst, candidate):\n",
    "    if oldWorst<candidate:\n",
    "        return oldWorst\n",
    "    else:\n",
    "        return candidate\n",
    "    \n",
    "def addArray(array1, array2):\n",
    "    answer=[a1 + a2 for a1, a2 in zip(array1, array2)]\n",
    "    return answer\n",
    "    \n",
    "    \n",
    "def applyingAllEstimators (estimators,numberOfRandomStates,data):   \n",
    "    # Initializing the variables\n",
    "    # Averages\n",
    "    p_svm=0\n",
    "    p_dt=0\n",
    "    p_rf=0\n",
    "    p_knn=0\n",
    "    p_nn=0\n",
    "    # Worst cases \n",
    "    w_knn=100\n",
    "    w_svm=100\n",
    "    w_rf=100\n",
    "    w_dt=100\n",
    "    w_nn=100\n",
    "    # Best cases\n",
    "    b_svm=0\n",
    "    b_dt=0\n",
    "    b_rf=0\n",
    "    b_knn=0\n",
    "    b_nn=0\n",
    "    \n",
    "    #Other stats\n",
    "    stats_knn=np.array([0,0,0,0])\n",
    "    stats_svm=np.array([0,0,0,0])\n",
    "    stats_dt=np.array([0,0,0,0])\n",
    "    stats_rf=np.array([0,0,0,0])\n",
    "    stats_nn=np.array([0,0,0,0])\n",
    "    \n",
    "    for (i) in range(0,numberOfRandomStates):\n",
    "        #r1=np.random.randint(1,10001)\n",
    "        #r2=np.random.randint(1,10001)\n",
    "        r1=i\n",
    "        r2=i\n",
    "        # Splitting test data and train data\n",
    "        [train, test, indexes, y_test]=splitingData(randomState=i, data=data) \n",
    "        print(\"{}: RandomState r1={} and r2={}\".format(i, r1,r2))\n",
    "        # Fitting the KNN to the training set\n",
    "        stats=applyingEstimator(indexes=indexes,estimators=estimators[0],train=train, test=test, y_test=y_test)\n",
    "        p=stats[3]\n",
    "        stats_knn=addArray(stats_knn, stats)\n",
    "        p_knn=p_knn+p\n",
    "        w_knn=theWorstScore(oldWorst=w_knn, candidate=p)\n",
    "        b_knn=theBestScore(oldBest=b_knn, candidate=p)\n",
    "        # Fitting the SVM to the training set\n",
    "        estimators[1]=changingRandomState(estimators=estimators[1], randomState=r2)\n",
    "        stats=applyingEstimator(indexes=indexes,estimators=estimators[1],train=train, test=test, y_test=y_test)\n",
    "        p=stats[3]\n",
    "        stats_svm=addArray(stats_svm, stats)\n",
    "        p_svm=p_svm+p\n",
    "        w_svm=theWorstScore(oldWorst=w_svm, candidate=p)\n",
    "        b_svm=theBestScore(oldBest=b_svm, candidate=p)\n",
    "        # Fitting the DT to the training set\n",
    "        estimators[2]=changingRandomState(estimators=estimators[2], randomState=r2)\n",
    "        stats=applyingEstimator(indexes=indexes,estimators=estimators[2],train=train, test=test, y_test=y_test)\n",
    "        p=stats[3]\n",
    "        stats_dt=addArray(stats_dt, stats)\n",
    "        p_dt=p_dt+p\n",
    "        w_dt=theWorstScore(oldWorst=w_dt, candidate=p)\n",
    "        b_dt=theBestScore(oldBest=b_dt, candidate=p)\n",
    "        # Fitting the RF to the training set\n",
    "        estimators[3]=changingRandomState(estimators=estimators[3], randomState=r2)\n",
    "        stats=applyingEstimator(indexes=indexes,estimators=estimators[3],train=train, test=test, y_test=y_test)\n",
    "        p=stats[3]\n",
    "        stats_rf=addArray(stats_rf, stats)\n",
    "        p_rf=p_rf+p\n",
    "        w_rf=theWorstScore(oldWorst=w_rf, candidate=p)\n",
    "        b_rf=theBestScore(oldBest=b_rf, candidate=p)\n",
    "        # Fitting the NN to the training set\n",
    "        estimators[4]=changingRandomState(estimators=estimators[4], randomState=r2)\n",
    "        stats=applyingEstimator(indexes=indexes,estimators=estimators[4],train=train, test=test, y_test=y_test)\n",
    "        p=stats[3]\n",
    "        stats_nn=addArray(stats_nn, stats)\n",
    "        p_nn=p_nn+p\n",
    "        w_nn=theWorstScore(oldWorst=w_nn, candidate=p)\n",
    "        b_nn=theBestScore(oldBest=b_nn, candidate=p)\n",
    "        \n",
    "    stats_knn=[elem / numberOfRandomStates for elem in stats_knn]\n",
    "    stats_svm=[elem / numberOfRandomStates for elem in stats_svm]\n",
    "    stats_dt=[elem / numberOfRandomStates for elem in stats_dt]\n",
    "    stats_rf=[elem / numberOfRandomStates for elem in stats_rf]\n",
    "    stats_nn=[elem / numberOfRandomStates for elem in stats_nn]\n",
    "    \n",
    "    stats_knn.extend([b_knn, w_knn])\n",
    "    stats_svm.extend([b_svm, w_svm])\n",
    "    stats_dt.extend([b_dt, w_dt])\n",
    "    stats_rf.extend([b_rf, w_rf])\n",
    "    stats_nn.extend([b_nn, w_nn])\n",
    "    stats=np.array([stats_knn,stats_svm, stats_dt, stats_rf, stats_nn])\n",
    "    \n",
    "    tp=throughput(rxBytes=stats[:,1], downloadTime=stats[:,0])\n",
    "    s=pd.DataFrame({'Algorithm':['KNN', 'SVM', 'DT', 'RF', 'NN'],'Average Score':stats[:,3], 'Best score': stats[:,4], 'Worst score': stats[:,5],'Downloads Completed':stats[:,2],'Download Time':stats[:,0], 'Received Bytes': stats[:,1], 'Throughput': tp})\n",
    "    return s   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions part 3\n",
    "def calculatingStatistics(indexes, choice):\n",
    "    downloadTime=[]\n",
    "    rxBytes=[]\n",
    "    downloadsCompleted=0\n",
    "    for i in range(0, 200):\n",
    "        index=indexes[i]\n",
    "        if(choice[i]==0):\n",
    "            downloadTime.extend([data.downloadTimeT2[index]])\n",
    "            rxBytes.extend([data.rxBytesT2[index]])\n",
    "        else:\n",
    "            downloadTime.extend([data.downloadTimeT3[index]])\n",
    "            rxBytes.extend([data.rxBytesT3[index]])\n",
    "        if(downloadTime[i]!=100):\n",
    "            downloadsCompleted=downloadsCompleted+1\n",
    "    meanDownloadTime=np.mean(downloadTime) \n",
    "    meanRxBytes=np.mean(rxBytes)\n",
    "    downloadsCompleted=downloadsCompleted/2 # Divided by 200 and multiplied by 100\n",
    "    return[meanDownloadTime, meanRxBytes, downloadsCompleted]\n",
    "\n",
    "float_formatter = lambda num: \"%.2f\" % num\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "pd.options.display.float_format = float_formatter\n",
    "\n",
    "def throughput(rxBytes, downloadTime):\n",
    "    tp=[i/j for i,j in zip(rxBytes, downloadTime)]\n",
    "    return tp\n",
    "\n",
    "def featureScaling(x_train,x_test):\n",
    "    # Feature scaling (normalizing or stadardization of the scales)\n",
    "    # Helps the conversion of the algorithm\n",
    "    sc_X = StandardScaler()\n",
    "    x_train = sc_X.fit_transform(x_train)\n",
    "    x_test = sc_X.transform(x_test) # There is no need to fit after the training set is fit\n",
    "    return[x_train, x_test]\n",
    "\n",
    "def splitingData(data, randomState):\n",
    "    # Splitting the data between independent variables and dependent variable\n",
    "    y = data['best_output']\n",
    "    x = data[['rsrp1','downloadTimeT2','downloadTimeT3', 'rxBytesT2','rxBytesT3','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1','previousrsrq1','previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "\n",
    "    # Splitting test data and train data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.20, random_state = randomState)\n",
    "    indexes=x_train.axes[0]\n",
    "    indexes=indexes.values\n",
    "\n",
    "    data_train=data.loc[indexes]\n",
    "    data_test=pd.concat([data, data_train]).drop_duplicates(keep=False)\n",
    "    \n",
    "    indexes=x_test.axes[0]\n",
    "\n",
    "    # Level 1\n",
    "    x1 = data_train[['rsrp1','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1',\n",
    "                     'previousrsrq1','previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "    y1_t2 = (data_train.downloadTimeT2<100).astype(int).values\n",
    "    y1_t3 = (data_train.downloadTimeT3<100).astype(int).values\n",
    "    x1_test=data_test[['rsrp1','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1',\n",
    "                       'previousrsrq1','previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "    y1_t2_test = (data_test.downloadTimeT2<100).astype(int).values\n",
    "    y1_t3_test = (data_test.downloadTimeT3<100).astype(int).values\n",
    "    [x1, x1_test]=featureScaling(x1,x1_test)  \n",
    "\n",
    "    # Level 2A\n",
    "    # Gets only the data of the simulations when both eNBs are able to finish the download\n",
    "    x2 = data_train[['rsrp1','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1','previousrsrq1',\n",
    "                     'previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "    y2_t2 = data_train.downloadTimeT2.values\n",
    "    y2_t3= data_train.downloadTimeT3.values\n",
    "    x2_test = data_test[['rsrp1','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1',\n",
    "                         'previousrsrq1','previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "    y2_t2_test = data_test.downloadTimeT2.values\n",
    "    y2_t3_test= data_test.downloadTimeT3.values\n",
    "    [x2, x2_test]=featureScaling(x2,x2_test) \n",
    "\n",
    "    # Level 2B\n",
    "    # Gets only the data of the simulations none of the eNBs are able to finish the download\n",
    "    x3 = data_train[['rsrp1','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1','previousrsrq1',\n",
    "                     'previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "    y3_t2 = data_train.rxBytesT2.values\n",
    "    y3_t3= data_train.rxBytesT3.values\n",
    "    x3_test = data_test[['rsrp1','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1','previousrsrq1',\n",
    "                         'previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "    y3_t2_test = data_test.rxBytesT2.values\n",
    "    y3_t3_test= data_test.rxBytesT3.values\n",
    "    [x3, x3_test]=featureScaling(x3,x3_test) \n",
    "\n",
    "    train=[[x1, x2, x3], [y1_t2, y2_t2, y3_t2],[y1_t3, y2_t3, y3_t3]]\n",
    "    test= [[x1_test, x2_test, x3_test], [y1_t2_test, y2_t2_test, y3_t2_test],[y1_t3_test, y2_t3_test, y3_t3_test]]\n",
    "\n",
    "    \n",
    "    return [train, test, indexes, y_test] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions part 4\n",
    "# Estrategy 1 function\n",
    "\n",
    "def predictOutput(x, estimators):\n",
    "    # se o downloadTime previsto é menor que 100 para T2 e T3, ambos os targets completam o download\n",
    "    pred=np.array([[estimators[0,0].predict(x), estimators[0,1].predict(x)], [estimators[1,0].predict(x),estimators[1,1].predict(x)],\n",
    "                  [estimators[2,0].predict(x), estimators[2,1].predict(x)]])\n",
    "    out=[]\n",
    "    p00=pred[0,0]\n",
    "    p01=pred[0,1]\n",
    "    p10=pred[1,0]\n",
    "    p11=pred[1,1]\n",
    "    p20=pred[2,0]\n",
    "    p21=pred[2,1]\n",
    "    for (i) in range(0, 200):\n",
    "        if ((p00[i]==1)&(p01[i]==1)):\n",
    "            # Nível 2B\n",
    "            # o melhor output será aquele para o qual o downloadTime previsto é menor\n",
    "            if (p10[i]<=p11[i]):\n",
    "                out.extend([0])\n",
    "            else:\n",
    "                out.extend([1])\n",
    "\n",
    "        # se o downloadTime previsto é menor que 100 apenas para um dos targets, só um deles completa o download\n",
    "        elif ((p00[i]==1)|(p01[i]==1)):\n",
    "            # o melhor output será aquele para o qual o downloadTime previsto é menor que 100\n",
    "            if (p00[i]==1):\n",
    "                out.extend([0])\n",
    "            else:\n",
    "                out.extend([1])\n",
    "\n",
    "        # se o downloadTime previsto é igual a 100 para ambos os targets\n",
    "        elif ((p00[i]==0)&(p01[i]==0)):\n",
    "            # o melhor output será aquele para o qual o rxBytes previsto é maior\n",
    "            if (p20[i]>=p21[i]):\n",
    "                out.extend([0]) \n",
    "            else:\n",
    "                out.extend([1])\n",
    "    return out\n",
    "\n",
    "def applyingEstimator(estimators, train, test, indexes, y_test):\n",
    "    x=np.array(train[0])\n",
    "    y=np.array(train[1:3])\n",
    "    x_test=np.array(test[0])\n",
    "    #Fitting the estimators\n",
    "    ## Level 1\n",
    "    estimators[0,0].fit(x[0],y[0,0])\n",
    "    estimators[0,1].fit(x[0],y[1,0])\n",
    "    ## Level 2A\n",
    "    estimators[1,0].fit(x[1],y[0,1])\n",
    "    estimators[1,1].fit(x[1],y[1,1])\n",
    "    ## Level 2B\n",
    "    estimators[2,0].fit(x[2],y[0,2])\n",
    "    estimators[2,1].fit(x[2],y[1,2])\n",
    "    \n",
    "    y_pred=predictOutput(x=x_test[0], estimators=estimators)\n",
    "    stats=calculatingStatistics(indexes=indexes, choice=y_pred)\n",
    "    \n",
    "    print(out)\n",
    "    print(y_test)\n",
    "    \n",
    "    # Evaluation of the model\n",
    "    # Making the confusion matrix\n",
    "    #cm = confusion_matrix(y_test, y_pred)\n",
    "    p=percentageOfErrors(y_real=y_test, y_predicted=y_pred)\n",
    "    s=np.array([ stats[0], stats[1], stats[2], 100 - p])\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "def changingRandomState(estimators, randomState):\n",
    "    estimators[0,0].random_state=randomState\n",
    "    estimators[0,1].random_state=randomState\n",
    "    estimators[1,0].random_state=randomState\n",
    "    estimators[1,1].random_state=randomState\n",
    "    estimators[2,0].random_state=randomState\n",
    "    estimators[2,1].random_state=randomState\n",
    "    return estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sem shadowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "\n",
    "# importando os datasets\n",
    "t2 = pd.read_csv('resultados/t2_OkumuraHata_Modificado', delimiter='\\t')\n",
    "t3 = pd.read_csv('resultados/t3_OkumuraHata_Modificado', delimiter='\\t')\n",
    "\n",
    "# garantindo que utilizaremos apenas as sementes presentes nos dois datasets\n",
    "t2 = t2[t2.nRun.isin(t3.nRun)]\n",
    "t3 = t3[t3.nRun.isin(t2.nRun)]\n",
    "t2 = t2.reset_index(drop=True)\n",
    "t3 = t3.reset_index(drop=True)\n",
    "\n",
    "# combinando os datasets\n",
    "data = t2\n",
    "data = data.drop(['targetCellId', 'downloadTime', 'rxBytes'], axis=1)\n",
    "data['downloadTimeT2'] = t2.downloadTime\n",
    "data['downloadTimeT3'] = t3.downloadTime\n",
    "data['rxBytesT2'] = t2.rxBytes\n",
    "data['rxBytesT3'] = t3.rxBytes\n",
    "\n",
    "data['best_output'] = data.apply(best_output, axis=1)\n",
    "data.head()\n",
    "\n",
    "[train,test, indexes, y_test]=splitingData(data=data, randomState=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_knn=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=6, p=1,\n",
    "           weights='uniform')\n",
    "\n",
    "classifier_svm=SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovo', degree=3, gamma=1, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "\n",
    "classifier_dt=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
    "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
    "            splitter='best')\n",
    "\n",
    "\n",
    "classifier_rf=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "            criterion='gini', max_depth=None, max_features=None,\n",
    "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "            min_impurity_split=None, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
    "            verbose=0, warm_start=False)\n",
    "\n",
    "classifier_nn=MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
    "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=[10,10,10], learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
    "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "       warm_start=False)\n",
    "\n",
    "conf=[classifier_knn,classifier_svm,classifier_dt,classifier_rf, classifier_nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for NN estimator\n"
     ]
    }
   ],
   "source": [
    "# Comment this line if you want to use the estimators above\n",
    "conf=gettingAllEstimatorsConfigurations(train=train, randomState=0)\n",
    "\n",
    "# Setting the estimators\n",
    "estimator_knn=conf[0]\n",
    "estimator_svm=conf[1]\n",
    "estimator_dt=conf[2]\n",
    "estimator_rf=conf[3]\n",
    "estimator_nn=conf[4]\n",
    "\n",
    "\n",
    "estimators=np.array([estimator_knn,estimator_svm,estimator_dt,estimator_rf, estimator_nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the classifiers\n",
    "maxRange=100\n",
    "stats = applyingAllEstimators(data=data,estimators=estimators,numberOfRandomStates=maxRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results without shadowing:')\n",
    "print(stats)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Com shadowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "\n",
    "# importando os datasets\n",
    "t2 = pd.read_csv('resultados/t2_OhBuildings_ComShadowing_Modificado', delimiter='\\t')\n",
    "t3 = pd.read_csv('resultados/t3_OhBuildings_ComShadowing_Modificado', delimiter='\\t')\n",
    "\n",
    "# garantindo que utilizaremos apenas as sementes presentes nos dois datasets\n",
    "t2 = t2[t2.nRun.isin(t3.nRun)]\n",
    "t3 = t3[t3.nRun.isin(t2.nRun)]\n",
    "t2 = t2.reset_index(drop=True)\n",
    "t3 = t3.reset_index(drop=True)\n",
    "\n",
    "# combinando os datasets\n",
    "data = t2\n",
    "data = data.drop(['targetCellId', 'downloadTime', 'rxBytes'], axis=1)\n",
    "data['downloadTimeT2'] = t2.downloadTime\n",
    "data['downloadTimeT3'] = t3.downloadTime\n",
    "data['rxBytesT2'] = t2.rxBytes\n",
    "data['rxBytesT3'] = t3.rxBytes\n",
    "\n",
    "data['best_output'] = data.apply(best_output, axis=1)\n",
    "data.head()\n",
    "\n",
    "[train,test, indexes, y_test]=splitingData(data=data, randomState=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_nn=MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=[10, 10, 10], learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
    "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "       warm_start=False)\n",
    "\n",
    "classifier_knn=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=21, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "\n",
    "classifier_svm=SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
    "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "\n",
    "classifier_dt=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
    "            splitter='best')\n",
    "\n",
    "classifier_rf=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "            criterion='entropy', max_depth=None, max_features=None,\n",
    "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "            min_impurity_split=None, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=9, n_jobs=-1, oob_score=False, random_state=0,\n",
    "            verbose=0, warm_start=False)\n",
    "\n",
    "conf=[classifier_knn,classifier_svm,classifier_dt,classifier_rf, classifier_nn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this line if you want to use the estimators above\n",
    "conf=gettingAllEstimatorsConfigurations(train=train, randomState=0)\n",
    "\n",
    "# Setting the estimators\n",
    "estimator_knn=conf[0]\n",
    "estimator_svm=conf[1]\n",
    "estimator_dt=conf[2]\n",
    "estimator_rf=conf[3]\n",
    "estimator_nn=conf[4]\n",
    "\n",
    "\n",
    "estimators=np.array([estimator_knn,estimator_svm,estimator_dt,estimator_rf, estimator_nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying the classifiers\n",
    "maxRange=200\n",
    "stats = applyingAllClassifiers(numberOfRandomStates=maxRange, x=x, y=y, classifiers=classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Results with shadowing:')\n",
    "print(stats)\n",
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
